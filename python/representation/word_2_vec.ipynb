{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c742b88-9e34-40c1-8e4e-77192c12d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os melhores produtos são vendidos por uma loja que se preocupa com a experiência do cliente. Muitos adoram produtos que chegam no prazo ou que satisfazem suas expectativas. Pessoas não gostam de produtos vendidos com tamanhos incorretos, inadequados ou que chegam em atraso. Pessoas também amam produtos que atendem a todas as suas expectativas. \n",
      "Sentence tokens: \n",
      "['Os melhores produtos são vendidos por uma loja que se preocupa com a experiência do cliente.', 'Muitos adoram produtos que chegam no prazo ou que satisfazem suas expectativas.', 'Pessoas não gostam de produtos vendidos com tamanhos incorretos, inadequados ou que chegam em atraso.', 'Pessoas também amam produtos que atendem a todas as suas expectativas.']\n",
      "\n",
      "\n",
      "\n",
      "Word tokens: \n",
      "[['Os', 'melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'a', 'experiência', 'do', 'cliente', '.'], ['Muitos', 'adoram', 'produtos', 'que', 'chegam', 'no', 'prazo', 'ou', 'que', 'satisfazem', 'suas', 'expectativas', '.'], ['Pessoas', 'não', 'gostam', 'de', 'produtos', 'vendidos', 'com', 'tamanhos', 'incorretos', ',', 'inadequados', 'ou', 'que', 'chegam', 'em', 'atraso', '.'], ['Pessoas', 'também', 'amam', 'produtos', 'que', 'atendem', 'a', 'todas', 'as', 'suas', 'expectativas', '.']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lower case, punctuation free word tokens\n",
      "[['os', 'melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'a', 'experiência', 'do', 'cliente'], ['muitos', 'adoram', 'produtos', 'que', 'chegam', 'no', 'prazo', 'ou', 'que', 'satisfazem', 'suas', 'expectativas'], ['pessoas', 'não', 'gostam', 'de', 'produtos', 'vendidos', 'com', 'tamanhos', 'incorretos', 'inadequados', 'ou', 'que', 'chegam', 'em', 'atraso'], ['pessoas', 'também', 'amam', 'produtos', 'que', 'atendem', 'a', 'todas', 'as', 'suas', 'expectativas']]\n",
      "a\n",
      "['os', 'melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'a', 'experiência', 'do', 'cliente']\n",
      "a\n",
      "['pessoas', 'também', 'amam', 'produtos', 'que', 'atendem', 'a', 'todas', 'as', 'suas', 'expectativas']\n",
      "as\n",
      "['pessoas', 'também', 'amam', 'produtos', 'que', 'atendem', 'todas', 'as', 'suas', 'expectativas']\n",
      "os\n",
      "['os', 'melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'experiência', 'do', 'cliente']\n",
      "de\n",
      "['pessoas', 'não', 'gostam', 'de', 'produtos', 'vendidos', 'com', 'tamanhos', 'incorretos', 'inadequados', 'ou', 'que', 'chegam', 'em', 'atraso']\n",
      "do\n",
      "['melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'experiência', 'do', 'cliente']\n",
      "uma\n",
      "['melhores', 'produtos', 'são', 'vendidos', 'por', 'uma', 'loja', 'que', 'se', 'preocupa', 'com', 'experiência', 'cliente']\n",
      "\n",
      "\n",
      "\n",
      "Normalized text: \n",
      "[['melhores', 'produtos', 'são', 'vendidos', 'por', 'loja', 'que', 'se', 'preocupa', 'com', 'experiência', 'cliente'], ['muitos', 'adoram', 'produtos', 'que', 'chegam', 'no', 'prazo', 'ou', 'que', 'satisfazem', 'suas', 'expectativas'], ['pessoas', 'não', 'gostam', 'produtos', 'vendidos', 'com', 'tamanhos', 'incorretos', 'inadequados', 'ou', 'que', 'chegam', 'em', 'atraso'], ['pessoas', 'também', 'amam', 'produtos', 'que', 'atendem', 'todas', 'suas', 'expectativas']]\n",
      "[-0.01709669 -0.00378546 -0.02363535  0.00827081  0.02834421 -0.00742891\n",
      " -0.00305159  0.01097988  0.0270784  -0.01850579 -0.02152368 -0.00916558\n",
      "  0.02858655  0.00270709 -0.02712    -0.00452181  0.02962333 -0.02359215\n",
      " -0.01674406  0.02911426 -0.02804289  0.01195596  0.0020795   0.02081469\n",
      "  0.02597735 -0.0089087  -0.01247598  0.02780599  0.00653014  0.01952794\n",
      " -0.02951786  0.02996914]\n",
      "[-0.02995087 -0.03020483 -0.01921311 -0.00040178  0.00624193  0.0294749\n",
      "  0.0174511  -0.01340843  0.00086974  0.01551362  0.02405722 -0.0035757\n",
      "  0.01351069 -0.01816994 -0.0025131   0.02531266 -0.0073752  -0.0301983\n",
      "  0.01806019 -0.0122807  -0.00382148  0.03118912 -0.0070511  -0.01486583\n",
      " -0.01665433  0.02181528 -0.01784023  0.0066052  -0.01642394  0.01912723\n",
      "  0.01361658  0.00814486]\n",
      "[-0.00971558  0.01767253  0.01812    -0.01554578  0.00241666 -0.02654931\n",
      "  0.02440564  0.02892904 -0.00856977  0.0025007   0.00233329  0.01711839\n",
      " -0.026894    0.00182642  0.02146694  0.00697373  0.00351461 -0.02913174\n",
      "  0.0265074  -0.0195754  -0.00935117  0.01091808 -0.00241446  0.00441029\n",
      "  0.00556872 -0.02134031 -0.03039004  0.02825183  0.01936892 -0.0216029\n",
      "  0.01063588  0.00064395]\n",
      "\n",
      " Cosine similarity between 'adoram' and 'amam': \n",
      "0.022087693\n",
      "Cosine similarity between 'amam' and 'atraso': \n",
      "-0.05651777\n",
      "['pessoas', 'adoram', 'produtos']\n",
      "\n",
      " \n",
      "\n",
      "Document word embeddings: \n",
      "[[-0.00471253  0.00771811 -0.00277508  0.01729269 -0.0085718   0.0070627\n",
      "   0.01704936  0.0260811  -0.00454294 -0.02877545  0.01365798  0.00178683\n",
      "   0.02325596 -0.00254151 -0.00824504 -0.02735315 -0.00267674  0.00883301\n",
      "   0.01687947  0.02203955 -0.01782225  0.00580881  0.0190277  -0.01499391\n",
      "  -0.00971019  0.02124259  0.00509836  0.00059349  0.01085512  0.00068055\n",
      "   0.03005883  0.01581439]\n",
      " [-0.01709669 -0.00378546 -0.02363535  0.00827081  0.02834421 -0.00742891\n",
      "  -0.00305159  0.01097988  0.0270784  -0.01850579 -0.02152368 -0.00916558\n",
      "   0.02858655  0.00270709 -0.02712    -0.00452181  0.02962333 -0.02359215\n",
      "  -0.01674406  0.02911426 -0.02804289  0.01195596  0.0020795   0.02081469\n",
      "   0.02597735 -0.0089087  -0.01247598  0.02780599  0.00653014  0.01952794\n",
      "  -0.02951786  0.02996914]\n",
      " [ 0.02980662 -0.0228724  -0.00729303 -0.00605544  0.02524199 -0.01853405\n",
      "   0.00014113 -0.01485542 -0.0300111   0.01564779 -0.02737371 -0.01372445\n",
      "  -0.00010969 -0.00092557 -0.02394138  0.03004607  0.01556893  0.02885357\n",
      "  -0.02549349  0.01404937 -0.01292836  0.00257668  0.02655819 -0.0139443\n",
      "   0.01411719 -0.02120925 -0.01108903  0.02937034 -0.00493016  0.00100429\n",
      "  -0.01293947 -0.0240084 ]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      " \n",
      "\n",
      "Document feature vector based upon word embeddings: \n",
      "[ 0.0026658  -0.00631325 -0.01123449  0.00650269  0.0150048  -0.00630008\n",
      "  0.00471297  0.00740186 -0.00249188 -0.01054448 -0.01174647 -0.0070344\n",
      "  0.01724428 -0.00025333 -0.01976881 -0.00060963  0.01417184  0.00469814\n",
      " -0.00845269  0.02173439 -0.01959784  0.00678048  0.01588846 -0.00270784\n",
      "  0.01012812 -0.00295845 -0.00615555  0.01925661  0.0041517   0.00707093\n",
      " -0.00413283  0.00725837]\n"
     ]
    }
   ],
   "source": [
    "# Basic word embedding representation.\n",
    "#\n",
    "# This example uses the neural network based gensim model.\n",
    "# For each word, a dense vector representation is obtained\n",
    "# according to word context\n",
    "#\n",
    "# Author: Fabrício Galende Marques de Carvalho\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import dot \n",
    "from numpy.linalg import norm\n",
    "from numpy import average\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_corpus = \"Os melhores produtos \" \\\n",
    "\"são vendidos por uma loja que se preocupa \"\\\n",
    "\"com a experiência do cliente. Muitos adoram \" \\\n",
    "\"produtos que chegam no prazo ou que satisfazem \"\\\n",
    "\"suas expectativas. Pessoas não gostam de produtos \"\\\n",
    "\"vendidos com tamanhos incorretos, inadequados \"\\\n",
    "\"ou que chegam em atraso. Pessoas também amam produtos \"\\\n",
    "\"que atendem a todas as suas expectativas. \"\n",
    "\n",
    "\n",
    "\n",
    "print(text_corpus)\n",
    "tokenizer = sent_tokenize\n",
    "# 1. Basic sentence tokens building\n",
    "sentence_tokens = tokenizer(text_corpus)\n",
    "print(\"Sentence tokens: \")\n",
    "print(sentence_tokens)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# 2. Basic word tokens building\n",
    "word_tokens = [ word_tokenize(sentence)  for sentence in sentence_tokens]\n",
    "print(\"Word tokens: \")\n",
    "print(word_tokens)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# 3. Removing punctuation characters and converting\n",
    "# all charecters to lower case:\n",
    "normalized_sentences = []\n",
    "i = 0\n",
    "for sentence in word_tokens:\n",
    "    normalized_sentences.append([])\n",
    "    for word in sentence:\n",
    "        word = re.sub(r'[^A-Za-zÀ-Ýà-ý]','', word).lower()\n",
    "        if word!='':\n",
    "            normalized_sentences[i].append(word)\n",
    "    i = i+1\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Lower case, punctuation free word tokens\")\n",
    "print(normalized_sentences)\n",
    "\n",
    "# 4. Stop word removal:\n",
    "stop_words = ['a', 'as', 'e', 'o', 'os', 'da', 'de', 'do', 'um', 'uma']\n",
    "for word in stop_words:\n",
    "    for sentence in normalized_sentences:\n",
    "        if word in sentence:\n",
    "            print(word)\n",
    "            print(sentence)\n",
    "            sentence.remove(word)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Normalized text: \")\n",
    "print(normalized_sentences)\n",
    "\n",
    "# 5. building the word2vec model\n",
    "# Model configuration\n",
    "feature_size = 32  # size of vector representation\n",
    "window_context = 3\n",
    "min_word_count = 1\n",
    "sample = 1e-5\n",
    "w2vec_model = word2vec.Word2Vec(normalized_sentences, vector_size= feature_size,\n",
    "                                window=window_context, min_count= min_word_count,\n",
    "                                sample=sample, epochs = 50)\n",
    "\n",
    "\n",
    "v1 = w2vec_model.wv[\"adoram\"]\n",
    "v2 = w2vec_model.wv[\"amam\"]\n",
    "v3 = w2vec_model.wv[\"atraso\"]\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3)\n",
    "print(\"\\n Cosine similarity between 'adoram' and 'amam': \")\n",
    "print(dot(v1,v2)/(norm(v1)*norm(v2)))\n",
    "\n",
    "print(\"Cosine similarity between 'amam' and 'atraso': \")\n",
    "print(dot(v1,v3)/(norm(v1)*norm(v3)))\n",
    "\n",
    "# Now we model a single document that must have words present\n",
    "# in original corpus used to train the model. This document models,\n",
    "# for example, a single product review, etc.\n",
    "\n",
    "doc = \"pessoas adoram os produtos\"\n",
    "normalized_doc = doc.split()\n",
    "normalized_doc.remove('os')\n",
    "print(normalized_doc)\n",
    "word_embedding = [] \n",
    "for word in normalized_doc:\n",
    "    word_embedding.append(w2vec_model.wv[word])\n",
    "word_embedding = np.array(word_embedding)\n",
    "print(\"\\n \\n\")\n",
    "print(\"Document word embeddings: \")\n",
    "print(word_embedding)\n",
    "\n",
    "doc_embedding = np.zeros(feature_size)\n",
    "print(doc_embedding)\n",
    "for i in range(len(doc_embedding)):     \n",
    "    doc_embedding[i] = average(word_embedding[:,i])\n",
    "print(\"\\n \\n\")\n",
    "print(\"Document feature vector based upon word embeddings: \")\n",
    "print(doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce94ad0-4fff-4266-bb55-aef7f2719e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
