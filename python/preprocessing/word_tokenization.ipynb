{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ecaa9b-2ab2-4b69-9eb4-39b9c8dfcb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens (input):  ['Hoje de manhã Pedro acordou com muita fome.', 'Levantou, escovou os dentes, lavou o rosto e foi para a cozinha.', 'Abriu a geladeira, pegou uma jarra de suco e três ovos.', \"Tomou um copo d'água.\", 'Fritou os ovos.', 'Sentou na mesa e saboreou seu café da manhã...']\n",
      "\n",
      " Word tokens:  [['Hoje', 'de', 'manhã', 'Pedro', 'acordou', 'com', 'muita', 'fome', '.'], ['Levantou', ',', 'escovou', 'os', 'dentes', ',', 'lavou', 'o', 'rosto', 'e', 'foi', 'para', 'a', 'cozinha', '.'], ['Abriu', 'a', 'geladeira', ',', 'pegou', 'uma', 'jarra', 'de', 'suco', 'e', 'três', 'ovos', '.'], ['Tomou', 'um', 'copo', \"d'água\", '.'], ['Fritou', 'os', 'ovos', '.'], ['Sentou', 'na', 'mesa', 'e', 'saboreou', 'seu', 'café', 'da', 'manhã', '...']]\n",
      "\n",
      " First word token extracted from first sentence:  Hoje\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Word tokenization example.\n",
    "Tokens are independent and minimal textual\n",
    "components which are related to some syntax\n",
    "and semantics. In this example we get word tokens\n",
    "from a list of sentences\n",
    "Author: Fabrício Galende M. de Carvalho, DSc\n",
    "'''\n",
    "import  nltk\n",
    "sentence_tokens = [\"Hoje de manhã Pedro acordou com muita fome.\",\n",
    "                   \"Levantou, escovou os dentes, lavou o rosto e foi para a cozinha.\",\n",
    "                   \"Abriu a geladeira, pegou uma jarra de suco e três ovos.\", \n",
    "                   \"Tomou um copo d'água.\",\n",
    "                   \"Fritou os ovos.\",\n",
    "                   \"Sentou na mesa e saboreou seu café da manhã...\"]\n",
    "\n",
    "default_nltk_word_tokenizer = nltk.word_tokenize\n",
    "word_tokens = []\n",
    "for sentence in sentence_tokens:\n",
    "    word_tokens.append( default_nltk_word_tokenizer(sentence))\n",
    "print(\"Sentence Tokens (input): \", sentence_tokens)\n",
    "print(\"\\n Word tokens: \", word_tokens)\n",
    "print(\"\\n First word token extracted from first sentence: \", word_tokens[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786aef06-234d-4de7-9e64-ef00b10720ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document:  Hoje de manhã Pedro acordou com muita fome.Levantou, escovou os dentes, lavou o rosto e foi para a cozinha.Abriu a geladeira, pegou uma jarra de suco e três ovos.Tomou um copo d'água.Fritou os ovos.Sentou na mesa e saboreou seu café da manhã...\n",
      "\n",
      "Sentences:  ['Hoje de manhã', 'Pedro acordou com muita fome.', 'Levantou, escovou os dentes, lavou o rosto e foi para a cozinha.', 'Abriu a geladeira, pegou uma jarra de suco e três ovos.', \"Tomou um copo d'água.\", 'Fritou os ovos.', 'Sentou na mesa e saboreou seu café da manhã...']\n",
      "\n",
      "Word tokens:  [Hoje, de, manhã, Pedro, acordou, com, muita, fome, ., Levantou, ,, escovou, os, dentes, ,, lavou, o, rosto, e, foi, para, a, cozinha, ., Abriu, a, geladeira, ,, pegou, uma, jarra, de, suco, e, três, ovos, ., Tomou, um, copo, d'água, ., Fritou, os, ovos, ., Sentou, na, mesa, e, saboreou, seu, café, da, manhã, ...]\n"
     ]
    }
   ],
   "source": [
    "# Now we test a basic word tokenization using Spacy\n",
    "# In this case, the object used to access word tokens \n",
    "# operates over the entire document. To access individual\n",
    "# sentences, property sents shall be used.\n",
    "\n",
    "import spacy\n",
    "document = ''\n",
    "for sentence in sentence_tokens:\n",
    "    document += \"\"+sentence\n",
    "print(\"Original document: \", document)\n",
    "\n",
    "model_spacy = spacy.load('pt_core_news_sm')\n",
    "print(\"\\nSentences: \", [ t.text for t in spacy_object.sents]) # to build sentences from document \n",
    "\n",
    "spacy_object = model_spacy(document) #sentence_tokens[0])\n",
    "word_tokens = [ item for item in spacy_object]\n",
    "print(\"\\nWord tokens: \", word_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fe3e2-4840-42b4-98f0-70920ebbaef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "natural_language_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
