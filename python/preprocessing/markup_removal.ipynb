{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a751bcf-eb77-41cf-94e3-e3643f4acdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response text: \n",
      "\n",
      "<body>\n",
      "<p>The product is good!</p>\n",
      "<p>It was delivered on time. </p>\n",
      "<p>Definetely I will buy it again! </p>\n",
      "<script> var x = 1; </script>\n",
      "<iframe> contents... </iframe>\n",
      "<p>Thanks folks! </p>\n",
      "</body>\n",
      "\n",
      "[<p>The product is good!</p>, <p>It was delivered on time. </p>, <p>Definetely I will buy it again! </p>, <p>Thanks folks! </p>]\n",
      "['The product is good!', 'It was delivered on time. ', 'Definetely I will buy it again! ', 'Thanks folks! ']\n"
     ]
    }
   ],
   "source": [
    "# This example shows how to remove markup elements\n",
    "# and extract only the relevant content.\n",
    "#\n",
    "# In this notebook, the BeautifulSoup component is used in\n",
    "# the first two examples. The third example uses regular\n",
    "# expressions to filter data. The last example combines BeautifulSoup,\n",
    "# regular expressions and requests. All preprocessing related to\n",
    "# markup removal is now wrapped by a function.\n",
    "#\n",
    "# Author: Fabr√≠cio Galende M. de Carvalho\n",
    "#\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Request response models a simple http request raw response. \n",
    "request_response_text = '''\n",
    "<body>\n",
    "<p>The product is good!</p>\n",
    "<p>It was delivered on time. </p>\n",
    "<p>Definetely I will buy it again! </p>\n",
    "<script> var x = 1; </script>\n",
    "<iframe> contents... </iframe>\n",
    "<p>Thanks folks! </p>\n",
    "</body>\n",
    "'''\n",
    "\n",
    "print(\"Raw response text: \")\n",
    "print(request_response_text)\n",
    "\n",
    "soup_obj = BeautifulSoup(request_response_text, \"html.parser\")\n",
    "paragraphs = soup_obj.find_all(\"p\")\n",
    "print(paragraphs)\n",
    "paragraphs_contents = [ p.text for p in paragraphs]\n",
    "print(paragraphs_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b890b7d6-4ecd-47af-ba65-7bd530b31028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get all elements after destroying irrelevant ones.\n",
      "[<body>\n",
      "<p>The product is good!</p>\n",
      "<p>It was delivered on time. </p>\n",
      "<p>Definetely I will buy it again! </p>\n",
      "\n",
      "<div>This content is also poorly structured. </div>\n",
      "\n",
      "This paragraph is not inside any tag, so, simple tag text finding will not work here.\n",
      "<p>Thanks folks! </p>\n",
      "</body>, <p>The product is good!</p>, <p>It was delivered on time. </p>, <p>Definetely I will buy it again! </p>, <div>This content is also poorly structured. </div>, <p>Thanks folks! </p>]\n",
      "\n",
      "\n",
      "\n",
      " *********** \n",
      "Content after unwrapping\n",
      "\n",
      "\n",
      "The product is good!\n",
      "It was delivered on time. \n",
      "Definetely I will buy it again! \n",
      "\n",
      "This content is also poorly structured. \n",
      "\n",
      "This paragraph is not inside any tag, so, simple tag text finding will not work here.\n",
      "Thanks folks! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, suppose that the HTML is not well formed. In this case, one\n",
    "# must extract all tags that do not carry relevant textual content\n",
    "request_response_text_2 = '''\n",
    "<body>\n",
    "<p>The product is good!</p>\n",
    "<p>It was delivered on time. </p>\n",
    "<p>Definetely I will buy it again! </p>\n",
    "<script> var x = 1; </script>\n",
    "<div>This content is also poorly structured. </div>\n",
    "<iframe> This content is not relevant. </iframe>\n",
    "This paragraph is not inside any tag, so, simple tag text finding will not work here.\n",
    "<p>Thanks folks! </p>\n",
    "</body>\n",
    "'''\n",
    "soup_obj_2 = BeautifulSoup(request_response_text_2, \"html.parser\")\n",
    "for element in soup_obj_2([\"script\", \"style\", \"meta\", \"link\", \"iframe\"]):\n",
    "    element.decompose() #destroys elements in a recursive way\n",
    "\n",
    "print(\"Get all elements after destroying irrelevant ones.\")\n",
    "print(soup_obj_2.find_all(True))\n",
    "\n",
    "# Here we unwrap the content (remaining tag removal)\n",
    "for element in soup_obj_2.find_all(True):\n",
    "    element.unwrap()\n",
    "\n",
    "print(\"\\n\\n\\n *********** \")\n",
    "print(\"Content after unwrapping\")\n",
    "print(soup_obj_2.get_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e62d74a-c1fe-441e-981d-c068e3e75897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The product is good!', 'It was delivered on time. ', 'Definetely I will buy it again! ', 'Thanks folks! ']\n"
     ]
    }
   ],
   "source": [
    "# Now, we use regular expression to get the relevant content\n",
    "import re\n",
    "request_response_text_3 = '''\n",
    "<body>\n",
    "<p>The product is good!</p>\n",
    "<p>It was delivered on time. </p>\n",
    "<p>Definetely I will buy it again! </p>\n",
    "<script> var x = 1; </script>\n",
    "<div>This content is also poorly structured. </div>\n",
    "<iframe> This content is not relevant. </iframe>\n",
    "<p>Thanks folks! </p>\n",
    "</body>\n",
    "'''\n",
    "\n",
    "# . any character except \\n, * one or more occurence of previous element, \n",
    "# () captures only the data inside <p> </p>, i.e., its a group\n",
    "regex_filter = r'<p>(.*?)</p>'  \n",
    "relevant_content = re.findall(regex_filter, request_response_text_3)\n",
    "print(relevant_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe5e86f-fdf5-4400-9c3a-5bd88240d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "\n",
      "you will have to check the laws of the country where you are located\n",
      "\n",
      "before using this eBook.\n",
      "Title: The Bible, King James version, Book 1: Genesis\n",
      "\n",
      "Author: Anonymous\n",
      "\n",
      "Release date: April 1, 2005 [eBook #8001]\n",
      "\n",
      "                Most recently updated: December 26, 2020\n",
      "Language: English\n",
      "Credits: This eBook was produced by David Widger with the help of Derek Andrew's text from January 1992 and the work of Bryan Taylor in November 2002\n",
      "\n",
      "*** START OF THE PROJE\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# a function to get rid of html tags\n",
    "def get_rid_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # iframe and script nodes removal from doc tree\n",
    "    [s.extract() for s in soup(['iframe','script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]','\\n',stripped_text) #replace some special characters with new line\n",
    "    return stripped_text\n",
    "\n",
    "data = requests.get(\"http://gutenberg.org/cache/epub/8001/pg8001.html\")\n",
    "content = data.content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "clean_content = get_rid_html_tags(soup.get_text())\n",
    "print(clean_content[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc6332-98c6-4987-a3d8-3999640c5d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
