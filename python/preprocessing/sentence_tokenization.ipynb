{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c546a472-c8da-4c14-bb29-85db2fa521c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text input:  \n",
      "Hoje de manhã Pedro acordou com muita fome. Levantou,\n",
      "escovou os dentes, lavou o rosto e foi para a cozinha.\n",
      "Abriu a geladeira, pegou uma jarra de suco e três ovos.\n",
      "Tomou um copo d'água. Fritou os ovos. Sentou na mesa e saboreou seu café da \n",
      "manhã...\n",
      "\n",
      "Tokenizer output: \n",
      "Total sentence tokens:  6\n",
      "Token # 1 : \n",
      "Hoje de manhã Pedro acordou com muita fome.\n",
      "Token # 2 : Levantou,\n",
      "escovou os dentes, lavou o rosto e foi para a cozinha.\n",
      "Token # 3 : Abriu a geladeira, pegou uma jarra de suco e três ovos.\n",
      "Token # 4 : Tomou um copo d'água.\n",
      "Token # 5 : Fritou os ovos.\n",
      "Token # 6 : Sentou na mesa e saboreou seu café da \n",
      "manhã...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokenization example.\n",
    "Tokens are independent and minimal textual\n",
    "components which are related to some syntax\n",
    "and semantics.\n",
    "Author: Fabrício Galende M. de Carvalho, DSc\n",
    "'''\n",
    "import nltk\n",
    "\n",
    "def print_tokens(tk):\n",
    "    tc = 1\n",
    "    print('Total sentence tokens: ', len(tk))\n",
    "    for token in tk:\n",
    "        print(\"Token #\", tc, \":\", token)\n",
    "        tc += 1\n",
    "\n",
    "sample_text_pt = \"\"\"\n",
    "Hoje de manhã Pedro acordou com muita fome. Levantou,\n",
    "escovou os dentes, lavou o rosto e foi para a cozinha.\n",
    "Abriu a geladeira, pegou uma jarra de suco e três ovos.\n",
    "Tomou um copo d'água. Fritou os ovos. Sentou na mesa e saboreou seu café da \n",
    "manhã...\n",
    "\"\"\"\n",
    "nltk_tokenizer =nltk.sent_tokenize\n",
    "sentence_tokens = nltk_tokenizer(text=sample_text_pt)\n",
    "print(\"Raw text input: \", sample_text_pt)\n",
    "print(\"Tokenizer output: \")\n",
    "print_tokens(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecaa9b-2ab2-4b69-9eb4-39b9c8dfcb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "natural_language_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
