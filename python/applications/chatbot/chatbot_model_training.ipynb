{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcd6f91-cd95-42ed-a6b1-7443c95ea731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens: \n",
      "['quem é o responsável pelo sistema?', 'qual a informação de contato?', 'qual o e-mail ou telefone? ', 'quais os produtos que são vendidos? ', 'vende carros novos e usados? ', 'aluga ou vende carros? ', 'oi!', 'olá, como vai você?', 'bom dia!']\n",
      "\n",
      "\n",
      "\n",
      "Word tokens: \n",
      "[['quem', 'é', 'o', 'responsável', 'pelo', 'sistema', '?'], ['qual', 'a', 'informação', 'de', 'contato', '?'], ['qual', 'o', 'e-mail', 'ou', 'telefone', '?'], ['quais', 'os', 'produtos', 'que', 'são', 'vendidos', '?'], ['vende', 'carros', 'novos', 'e', 'usados', '?'], ['aluga', 'ou', 'vende', 'carros', '?'], ['oi', '!'], ['olá', ',', 'como', 'vai', 'você', '?'], ['bom', 'dia', '!']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lower case, punctuation free word tokens\n",
      "[['quem', 'é', 'o', 'responsável', 'pelo', 'sistema'], ['qual', 'a', 'informação', 'de', 'contato'], ['qual', 'o', 'email', 'ou', 'telefone'], ['quais', 'os', 'produtos', 'que', 'são', 'vendidos'], ['vende', 'carros', 'novos', 'e', 'usados'], ['aluga', 'ou', 'vende', 'carros'], ['oi'], ['olá', 'como', 'vai', 'você'], ['bom', 'dia']]\n"
     ]
    }
   ],
   "source": [
    "# Here we train and save our chatbot classifier model\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "\n",
    "chat_topics = [\n",
    "    {\"user_input\": \"quem é o responsável pelo sistema?\", \"class\": \"contact\" , \"representation\": [] },\n",
    "    {\"user_input\": \"qual a informação de contato?\", \"class\": \"contact\" , \"representation\": [] },\n",
    "    {\"user_input\": \"qual o e-mail ou telefone? \", \"class\": \"contact\" , \"representation\": [] },\n",
    "    {\"user_input\": \"quais os produtos que são vendidos? \", \"class\": \"products\" , \"representation\": [] },\n",
    "    {\"user_input\": \"vende carros novos e usados? \", \"class\": \"products\" , \"representation\": [] },\n",
    "    {\"user_input\": \"aluga ou vende carros? \", \"class\": \"products\" , \"representation\": [] },\n",
    "    {\"user_input\": \"oi!\", \"class\": \"greetings\" , \"representation\": [] },\n",
    "    {\"user_input\": \"olá, como vai você?\", \"class\": \"greetings\" , \"representation\": [] },\n",
    "    {\"user_input\": \"bom dia!\", \"class\": \"greetings\" , \"representation\": [] }\n",
    "]              \n",
    "\n",
    "# now we build the normalized text without stop words, with lower case and without punctuation signals:\n",
    "topics = []\n",
    "for topic in chat_topics:\n",
    "    topics.append(topic[\"user_input\"])\n",
    "training_corpus = topics\n",
    "sentence_tokens = training_corpus \n",
    "print(\"Sentence tokens: \")\n",
    "print(sentence_tokens)\n",
    "print(\"\\n\\n\")\n",
    "# 1. Basic word tokens building\n",
    "word_tokens = [ word_tokenize(sentence)  for sentence in sentence_tokens]\n",
    "print(\"Word tokens: \")\n",
    "print(word_tokens)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# 2. Removing punctuation characters and converting\n",
    "# all charecters to lower case:\n",
    "normalized_sentences = []\n",
    "i = 0\n",
    "for sentence in word_tokens:\n",
    "    normalized_sentences.append([])\n",
    "    for word in sentence:\n",
    "        word = re.sub(r'[^A-Za-zÀ-Ýà-ý]','', word).lower()\n",
    "        if word!='':\n",
    "            normalized_sentences[i].append(word)\n",
    "    i = i+1\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Lower case, punctuation free word tokens\")\n",
    "print(normalized_sentences)\n",
    "\n",
    "# 3. Stop word removal: \n",
    "#stop_words = ['a', 'as', 'e', 'o', 'os', 'da', 'de', 'do', 'um', 'uma']\n",
    "#for word in stop_words:\n",
    "#    for sentence in normalized_sentences:\n",
    "#        if word in sentence:\n",
    "#            print(word)\n",
    "#            print(sentence)\n",
    "#            sentence.remove(word)\n",
    "#print(\"\\n\\n\")\n",
    "#print(\"Normalized text: \")\n",
    "#print(normalized_sentences)\n",
    "\n",
    "# 4. building the word2vec model\n",
    "# Model configuration\n",
    "feature_size = 32  # size of vector representation\n",
    "window_context = 3\n",
    "min_word_count = 1\n",
    "sample = 1e-3\n",
    "w2vec_repr = word2vec.Word2Vec(normalized_sentences, vector_size= feature_size,\n",
    "                                window=window_context, min_count= min_word_count,\n",
    "                                sample=sample, epochs = 50)\n",
    "\n",
    "# Here we save the word2vec model to be used in the chatbot:\n",
    "w2vec_repr.save(\"word2vec_bot.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b0cf93-c050-4955-ab6c-9bf104ad9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Now we build the feature vector for each user input example:\n",
    "for i, sentence in enumerate(normalized_sentences):\n",
    "    sentence_vector = np.zeros(w2vec_repr.vector_size)\n",
    "    for word in sentence:\n",
    "        sentence_vector += w2vec_repr.wv[word]\n",
    "    sentence_vector /= len(sentence)\n",
    "    chat_topics[i][\"representation\"] = sentence_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eef75f2-043e-45fa-a634-1947155b434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train our MLP model and save it using pickle\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X = []\n",
    "Y = []\n",
    "for topic in chat_topics:\n",
    "    X.append(topic[\"representation\"])\n",
    "    Y.append(topic[\"class\"])\n",
    "\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 3), random_state=1)\n",
    "\n",
    "classifier.fit(X, Y)\n",
    "# Now we save the model \n",
    "with open(\"mlp_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72140cf-c3c0-4b90-aa02-05c0c3c614c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['com', 'quem', 'eu', 'falo']\n",
      "[-2.07186881e-02  1.35313021e-02 -1.17272406e-03 -1.11813890e-02\n",
      "  2.14475952e-02  1.21138496e-02 -1.19936625e-02  2.41497671e-03\n",
      "  2.86840163e-02  2.43211538e-02  1.99904069e-02  1.45933731e-02\n",
      "  7.23376125e-03 -5.83335757e-03 -1.98978763e-02 -9.40478989e-04\n",
      " -4.71610902e-03 -1.93193590e-03 -1.96045060e-02  2.35680304e-02\n",
      " -2.04276610e-02 -2.25921944e-02 -8.39035027e-03 -4.63318499e-03\n",
      " -2.37647332e-02  2.05050455e-03 -1.68453883e-02 -4.01476864e-03\n",
      " -2.30164919e-02  5.89921512e-03  1.01045361e-02 -3.15128200e-05]\n",
      "['contact']\n",
      "[[9.92921799e-01 7.06321400e-03 1.49870802e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Let's test the model loading and classification:\n",
    "# First we load the model for representation and classification:\n",
    "\n",
    "word2vec_model_loaded = word2vec.Word2Vec.load(\"word2vec_bot.model\")\n",
    "with open(\"mlp_classifier.pkl\", \"rb\") as f:\n",
    "    loaded_classifier = pickle.load(f)\n",
    "    \n",
    "# Then we simulate user input:\n",
    "user_input = \"com quem eu falo? \"\n",
    "user_input_word_tokens = word_tokenize(user_input)\n",
    "user_input_normalized = []\n",
    "for word in user_input_word_tokens:\n",
    "        word = re.sub(r'[^A-Za-zÀ-Ýà-ý]','', word).lower()\n",
    "        if word!='':\n",
    "            user_input_normalized.append(word)\n",
    "print(user_input_normalized)\n",
    "\n",
    "#now we build the word2vec representation:\n",
    "input_sentence_vector = np.zeros(word2vec_model_loaded.vector_size)\n",
    "nwords = 0\n",
    "for word in user_input_normalized:\n",
    "    if word in word2vec_model_loaded.wv:\n",
    "        nwords +=1\n",
    "        input_sentence_vector += word2vec_model_loaded.wv[word]\n",
    "input_sentence_vector /= nwords\n",
    "print(input_sentence_vector)\n",
    "\n",
    "# Let's test classification:\n",
    "\n",
    "print( loaded_classifier.predict(input_sentence_vector.reshape(1, -1)) )\n",
    "print( loaded_classifier.predict_proba(input_sentence_vector.reshape(1, -1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce1eb1-5182-4cf0-b518-bbe646e6a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
