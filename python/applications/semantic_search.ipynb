{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf45a7e-0d4e-47af-99fa-774627707300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      docs category\n",
      "0  entrega rápida para são josé dos campos  entrega\n",
      "1         valor da entrega para meu bairro  entrega\n",
      "2                    comida muito saborosa   comida\n",
      "3         prato delicioso e muito saboroso   comida\n",
      "4       o custo de vida está muito elevado    custo\n",
      "5  custa caro morar em são josé dos campos    custo\n"
     ]
    }
   ],
   "source": [
    "# Basic semantic search example.\n",
    "# Semantics depends upon word context. Thus,\n",
    "# to perform semantic search a simple embedding\n",
    "# representation can be used for words.\n",
    "#\n",
    "# Author: Fabrício Galende Marques de Carvalho\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "documents= [\n",
    "    \"entrega rápida para são josé dos campos\",\n",
    "    \"valor da entrega para meu bairro\",\n",
    "    \"comida muito saborosa\",\n",
    "    \"prato delicioso e muito saboroso\",\n",
    "    \"o custo de vida está muito elevado\",\n",
    "    \"custa caro morar em são josé dos campos\"\n",
    "]\n",
    "\n",
    "# E algumas classes correspondentes\n",
    "labels = [\"entrega\", \"entrega\", \"comida\", \"comida\", \"custo\", \"custo\"]\n",
    "\n",
    "# Criar o DataFrame\n",
    "sentence_dataframe = DataFrame({  \"docs\": documents,\n",
    "    \"category\": labels})\n",
    "print(sentence_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4a5492-f062-435a-ae1b-f959ab0ac26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uma entrega rápida sempre deixa o cliente satisfeito e sempre é possível em são josé dos Campos. Custo depende do preço das coisas. Custo de vida alto sempre quer dizer coisas caras. Aquilo que comemos pode ser uma comida muito gostosa. Comida que comemos nos faz bem. Comida e alimento saudável sempre nos fazem bem. Alimento e comida boa é o que todos querem. \n",
      "Sentence tokens: \n",
      "['Uma entrega rápida sempre deixa o cliente satisfeito e sempre é possível em são josé dos Campos.', 'Custo depende do preço das coisas.', 'Custo de vida alto sempre quer dizer coisas caras.', 'Aquilo que comemos pode ser uma comida muito gostosa.', 'Comida que comemos nos faz bem.', 'Comida e alimento saudável sempre nos fazem bem.', 'Alimento e comida boa é o que todos querem.']\n",
      "\n",
      "\n",
      "\n",
      "Word tokens: \n",
      "[['Uma', 'entrega', 'rápida', 'sempre', 'deixa', 'o', 'cliente', 'satisfeito', 'e', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'Campos', '.'], ['Custo', 'depende', 'do', 'preço', 'das', 'coisas', '.'], ['Custo', 'de', 'vida', 'alto', 'sempre', 'quer', 'dizer', 'coisas', 'caras', '.'], ['Aquilo', 'que', 'comemos', 'pode', 'ser', 'uma', 'comida', 'muito', 'gostosa', '.'], ['Comida', 'que', 'comemos', 'nos', 'faz', 'bem', '.'], ['Comida', 'e', 'alimento', 'saudável', 'sempre', 'nos', 'fazem', 'bem', '.'], ['Alimento', 'e', 'comida', 'boa', 'é', 'o', 'que', 'todos', 'querem', '.']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lower case, punctuation free word tokens\n",
      "[['uma', 'entrega', 'rápida', 'sempre', 'deixa', 'o', 'cliente', 'satisfeito', 'e', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'campos'], ['custo', 'depende', 'do', 'preço', 'das', 'coisas'], ['custo', 'de', 'vida', 'alto', 'sempre', 'quer', 'dizer', 'coisas', 'caras'], ['aquilo', 'que', 'comemos', 'pode', 'ser', 'uma', 'comida', 'muito', 'gostosa'], ['comida', 'que', 'comemos', 'nos', 'faz', 'bem'], ['comida', 'e', 'alimento', 'saudável', 'sempre', 'nos', 'fazem', 'bem'], ['alimento', 'e', 'comida', 'boa', 'é', 'o', 'que', 'todos', 'querem']]\n",
      "e\n",
      "['uma', 'entrega', 'rápida', 'sempre', 'deixa', 'o', 'cliente', 'satisfeito', 'e', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'campos']\n",
      "e\n",
      "['comida', 'e', 'alimento', 'saudável', 'sempre', 'nos', 'fazem', 'bem']\n",
      "e\n",
      "['alimento', 'e', 'comida', 'boa', 'é', 'o', 'que', 'todos', 'querem']\n",
      "o\n",
      "['uma', 'entrega', 'rápida', 'sempre', 'deixa', 'o', 'cliente', 'satisfeito', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'campos']\n",
      "o\n",
      "['alimento', 'comida', 'boa', 'é', 'o', 'que', 'todos', 'querem']\n",
      "de\n",
      "['custo', 'de', 'vida', 'alto', 'sempre', 'quer', 'dizer', 'coisas', 'caras']\n",
      "do\n",
      "['custo', 'depende', 'do', 'preço', 'das', 'coisas']\n",
      "uma\n",
      "['uma', 'entrega', 'rápida', 'sempre', 'deixa', 'cliente', 'satisfeito', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'campos']\n",
      "uma\n",
      "['aquilo', 'que', 'comemos', 'pode', 'ser', 'uma', 'comida', 'muito', 'gostosa']\n",
      "\n",
      "\n",
      "\n",
      "Normalized text: \n",
      "[['entrega', 'rápida', 'sempre', 'deixa', 'cliente', 'satisfeito', 'sempre', 'é', 'possível', 'em', 'são', 'josé', 'dos', 'campos'], ['custo', 'depende', 'preço', 'das', 'coisas'], ['custo', 'vida', 'alto', 'sempre', 'quer', 'dizer', 'coisas', 'caras'], ['aquilo', 'que', 'comemos', 'pode', 'ser', 'comida', 'muito', 'gostosa'], ['comida', 'que', 'comemos', 'nos', 'faz', 'bem'], ['comida', 'alimento', 'saudável', 'sempre', 'nos', 'fazem', 'bem'], ['alimento', 'comida', 'boa', 'é', 'que', 'todos', 'querem']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from numpy import dot \n",
    "from numpy.linalg import norm\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_corpus = \"Uma entrega rápida sempre deixa o cliente satisfeito \" \\\n",
    "              \"e sempre é possível em são josé dos Campos. \"\\\n",
    "              \"Custo depende do preço das coisas. Custo de vida alto \" \\\n",
    "              \"sempre quer dizer coisas caras. Aquilo que comemos pode ser \" \\\n",
    "              \"uma comida muito gostosa. Comida que comemos nos faz bem. \"\\\n",
    "              \"Comida e alimento saudável sempre nos fazem bem. Alimento e \" \\\n",
    "              \"comida boa é o que todos querem. \"\n",
    "              \n",
    "print(text_corpus)\n",
    "tokenizer = sent_tokenize\n",
    "# 1. Basic sentence tokens building\n",
    "sentence_tokens = tokenizer(text_corpus)\n",
    "print(\"Sentence tokens: \")\n",
    "print(sentence_tokens)\n",
    "print(\"\\n\\n\")\n",
    "# 2. Basic word tokens building\n",
    "word_tokens = [ word_tokenize(sentence)  for sentence in sentence_tokens]\n",
    "print(\"Word tokens: \")\n",
    "print(word_tokens)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# 3. Removing punctuation characters and converting\n",
    "# all charecters to lower case:\n",
    "normalized_sentences = []\n",
    "i = 0\n",
    "for sentence in word_tokens:\n",
    "    normalized_sentences.append([])\n",
    "    for word in sentence:\n",
    "        word = re.sub(r'[^A-Za-zÀ-Ýà-ý]','', word).lower()\n",
    "        if word!='':\n",
    "            normalized_sentences[i].append(word)\n",
    "    i = i+1\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Lower case, punctuation free word tokens\")\n",
    "print(normalized_sentences)\n",
    "\n",
    "# 4. Stop word removal:\n",
    "stop_words = ['a', 'as', 'e', 'o', 'os', 'da', 'de', 'do', 'um', 'uma']\n",
    "for word in stop_words:\n",
    "    for sentence in normalized_sentences:\n",
    "        if word in sentence:\n",
    "            print(word)\n",
    "            print(sentence)\n",
    "            sentence.remove(word)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Normalized text: \")\n",
    "print(normalized_sentences)\n",
    "\n",
    "# 4. building the word2vec model\n",
    "# Model configuration\n",
    "feature_size = 32  # size of vector representation\n",
    "window_context = 3\n",
    "min_word_count = 1\n",
    "sample = 1e-3\n",
    "w2vec_repr = word2vec.Word2Vec(normalized_sentences, vector_size= feature_size,\n",
    "                                window=window_context, min_count= min_word_count,\n",
    "                                sample=sample, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a960aa-dca1-48d2-a401-970016b0ada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alimento' and 'entrega': \n",
      "0.049173858\n",
      "Cosine similarity between 'alimento' and 'comida': \n",
      "0.076921314\n",
      "Cosine similarity between 'alimento' and 'custo': \n",
      "-0.005212678\n",
      "\n",
      "Search result (i.e., API response o database query): \n",
      "Result: \n",
      "                               docs category\n",
      "2             comida muito saborosa   comida\n",
      "3  prato delicioso e muito saboroso   comida\n"
     ]
    }
   ],
   "source": [
    "# Let's check the semantic search:\n",
    "v1 = w2vec_repr.wv[\"alimento\"]\n",
    "v2 = w2vec_repr.wv[\"entrega\"]\n",
    "v3 = w2vec_repr.wv[\"comida\"]\n",
    "v4 = w2vec_repr.wv[\"custo\"]\n",
    "\n",
    "print(\"Cosine similarity between 'alimento' and 'entrega': \")\n",
    "print(dot(v1,v2)/(norm(v1)*norm(v2)))\n",
    "\n",
    "print(\"Cosine similarity between 'alimento' and 'comida': \")\n",
    "print(dot(v1,v3)/(norm(v1)*norm(v3)))\n",
    "\n",
    "print(\"Cosine similarity between 'alimento' and 'custo': \")\n",
    "print(dot(v1,v4)/(norm(v1)*norm(v4)))\n",
    "\n",
    "# If a database search is to be performed (suppose that pandas dataframe behaves like a db search)\n",
    "print(\"\\nSearch result (i.e., API response o database query): \")\n",
    "print(\"Result: \")\n",
    "print(sentence_dataframe[ sentence_dataframe[\"category\"] == \"comida\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3b7c5-e672-4574-9ff9-0f8989f21520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
